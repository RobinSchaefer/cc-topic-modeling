{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f337ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "from trankit import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec20d2e",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69bc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_json_climate_path = '/home/robin/research/phd/twitter_climate_politics/data/tweets/total/tweets_climate.json'\n",
    "#tweet_climate_concise_path = '/home/robin/research/phd/twitter_climate_politics/data/tweets/total/tweets_climate_concise.csv'\n",
    "tweet_climate_lemmatized_concise_path = '/home/robin/research/phd/twitter_climate_politics/data/tweets/total/tweets_climate_lemmatized_concise.csv'\n",
    "tweet_total_concise_path = '/home/robin/research/phd/twitter_climate_politics/data/tweets/total/tweets_total_concise.csv'\n",
    "\n",
    "topic_model_path = '/home/robin/research/phd/twitter_climate_politics/models/topic_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_climate_df = pd.read_csv(tweet_climate_concise_path, lineterminator='\\n')\n",
    "#print(tweets_climate_df.shape)\n",
    "\n",
    "tweets_climate_df = pd.read_csv(tweet_climate_lemmatized_concise_path, lineterminator='\\n') # includes lemmatized data\n",
    "print(tweets_climate_df.shape)\n",
    "tweets_total_df = pd.read_csv(tweet_total_concise_path, lineterminator='\\n')\n",
    "print(tweets_total_df.shape)\n",
    "\n",
    "tweets_climate = tweets_climate_df['text'].tolist()\n",
    "tweets_climate_lemmatized = tweets_climate_df['text_lemmatized'].tolist()\n",
    "tweets_total = tweets_total_df['text'].tolist()\n",
    "\n",
    "print('Tweets Climate Size: {}'.format(len(tweets_climate)))\n",
    "print('Tweets Climate Lemmatized Size: {}'.format(len(tweets_climate_lemmatized)))\n",
    "print('Tweets Total Size: {}'.format(len(tweets_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d91756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert floats to str\n",
    "for i in range(len(tweets_climate_lemmatized)):\n",
    "    tweet = tweets_climate_lemmatized[i]\n",
    "    if type(tweet) == float:\n",
    "        tweets_climate_lemmatized[i] = str(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb29d3",
   "metadata": {},
   "source": [
    "## 2. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d271643",
   "metadata": {},
   "source": [
    "### 2.1. Preprocess Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1080503",
   "metadata": {},
   "source": [
    "#### 2.1.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace\n",
    "tweets_climate = [tweet.strip() for tweet in tweets_climate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8202705",
   "metadata": {},
   "outputs": [],
   "source": [
    "trankit_pipeline = Pipeline('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c3f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tweets(tweets):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    lemmatized_tweets = []\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        if len(tweets[i]) > 0:\n",
    "            lemmatized_tweet = trankit_pipeline.lemmatize(tweets[i])\n",
    "            lemmas = []\n",
    "            for sent in lemmatized_tweet['sentences']:\n",
    "                for token in sent['tokens']:\n",
    "                    try:\n",
    "                        lemmas.append(token['lemma'])\n",
    "                    except:\n",
    "                        expanded_lemmas = [expanded_token['lemma'] for expanded_token in token['expanded']]\n",
    "                        lemmas.extend(expanded_lemmas)\n",
    "        \n",
    "            lemmatized_tweets.append(' '.join(lemmas))\n",
    "        else:\n",
    "            lemmatized_tweets.append(tweets[i])\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Lemmatized Tweets: {}'.format(i))\n",
    "    \n",
    "    \n",
    "    return lemmatized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_climate = lemmatize_tweets(tweets_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67854859",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_path = '/home/robin/research/phd/twitter_climate_politics/notebooks/data_test/lemmatized_tweets.pkl'\n",
    "\n",
    "with open(lemmatized_path, 'rb') as f_in:\n",
    "    tweets_climate = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e94071",
   "metadata": {},
   "source": [
    "#### 2.1.3 Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699f9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case\n",
    "tweets_climate = [tweet.lower() for tweet in tweets_climate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc368eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets_climate:\n",
    "    if 'parole' in tweet:\n",
    "        print(tweet)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef640a3f",
   "metadata": {},
   "source": [
    "#### 2.1.4 Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40018ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweets):\n",
    "    '''\n",
    "    '''\n",
    "    stopwords_german = get_stop_words('de').copy()\n",
    "    \n",
    "    ## extend stopwords\n",
    "    stopwords_german.extend(['amp', '&ampf', 'er|es|sie', 'sie|sie', 'er|es'])\n",
    "    stopwords_german.extend(['müssen', 'sollen', 'dürfen', 'lassen'])\n",
    "    stopwords_german.extend(['mehr', 'weniger'])\n",
    "    # add climate tokens to stopwords\n",
    "    #stopwords_german.extend(['klimaschutz', 'klima'])\n",
    "    # add climate keywords (used for tweet filtering)\n",
    "    #stopwords_german.extend(['cop26', 'cop26glasgow', 'klimagipfel', 'Weltklimagipfel', \n",
    "    #        'klimakonferenz', 'klimakrise', 'klimawandel',\n",
    "    #        'klimanotstand', 'klimahysterie', 'energiewende', 'fff'])\n",
    "    \n",
    "    umlaut_tokens = re.findall(r'\\w*[üöä]\\w+', ' '.join(stopwords_german))\n",
    "    no_umlaut_tokens = []\n",
    "\n",
    "    for umlaut_token in umlaut_tokens:\n",
    "        if 'ü' in umlaut_token:\n",
    "            no_umlaut_tokens.append(re.sub('ü', 'u', umlaut_token))\n",
    "        elif 'ö' in umlaut_token:\n",
    "            no_umlaut_tokens.append(re.sub('ö', 'o', umlaut_token))\n",
    "        elif 'ä' in umlaut_token:\n",
    "            no_umlaut_tokens.append(re.sub('ä', 'a', umlaut_token))\n",
    "    stopwords_german.extend(no_umlaut_tokens)\n",
    "    \n",
    "    stopwords_german = set(stopwords_german)\n",
    "    print('Number of Stopwords: {}'.format(len(stopwords_german)))\n",
    "    \n",
    "    ## remove stopwords\n",
    "    tweets_no_stopwords = []\n",
    "    \n",
    "    for tweet in tweets_climate:\n",
    "        tweet_no_stopwords = ' '.join([token for token in tweet.split() if not token in stopwords_german])\n",
    "        # remove 'er' following years, e.g. 1960er -> 1960\n",
    "        tweet_no_stopwords = re.sub(r'([0-9]+)er', r'\\1', tweet_no_stopwords)\n",
    "        tweets_no_stopwords.append(tweet_no_stopwords)\n",
    "    \n",
    "    return tweets_no_stopwords, stopwords_german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04175a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_climate, stopwords_german = remove_stopwords(tweets_climate)\n",
    "print(len(tweets_climate))          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57639079",
   "metadata": {},
   "source": [
    "### 2.2. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8502b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(tweets):\n",
    "    for tweet in tweets:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(tweet), deacc=False, min_len=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3dd7245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words = list(sent_to_words(tweets_climate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "21d33645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweets):\n",
    "    return [[word for word in simple_preprocess(str(tweet)) \n",
    "             if word not in stopwords_german] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "48acbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words = remove_stopwords(tweet_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14084901",
   "metadata": {},
   "source": [
    "### 2.3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25c17f",
   "metadata": {},
   "source": [
    "Topic Distribution\n",
    "\n",
    "P(topic t | post d) = number of words in post d allocated to topic t / total number of words in post d\n",
    "\n",
    "Word Distribution\n",
    "\n",
    "P(word w | topic t) = number of times word w is assigned to topic t in all posts in the collection / total number of occurrences of word w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2607b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6e15ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(words) for words in tweet_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9dd26b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "k = 5\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33df116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the k topics\n",
    "pprint(lda_model.print_topics(num_topics=k, num_words=10))\n",
    "doc_lda = lda_model[corpus]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
